{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bfd8262-7bf1-4198-a5f3-eb0486c53ca9",
   "metadata": {},
   "source": [
    "## LLM App/Project - RAG testing / Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27adbc05-d5ea-4ee8-ace5-b16c1bdfaf49",
   "metadata": {},
   "source": [
    "### Steps: \n",
    "\n",
    "1. ~~Create a chat classifier in lamaindex which based on the provided BBC headline classify it into one of the five news categories.~~\n",
    "2. Using the chat classifier from the first task, add output evaluation - consider a possiblity of adding the way of teaching/improving a chat output based on this information\n",
    "3. Write RAG - add some wiki documents (~ 20 documents) and evaluate the responses\n",
    "4. Experiment with different evaluation metrics\n",
    "5. Experiment with different RAG methodologies and use evaluation metrics to see different results they can provide\n",
    "\n",
    "Possible problems:\n",
    "- Gemini API restriction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7d890-ced9-4b35-a589-e98376f4c71a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chat Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b0d20c1-790d-44d8-aa76-85ec38c446a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "llm = Gemini(\n",
    "    model=\"models/gemini-1.5-flash\",\n",
    "    api_key=GOOGLE_API_KEY  # uses GOOGLE_API_KEY env var by default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3713355f-50d1-4267-93f2-4c0a826ccf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Response: assistant: Okay, I'm ready. Please provide the headline.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\",\n",
    "        content=(\n",
    "            \"You are a text classifier specializing in BBC headlines. \"\n",
    "            \"I will provide you with a headline, and you will classify it into one of the following five categories: \"\n",
    "            \"business, entertainment, politics, sport, or tech. \"\n",
    "            \"Choose the category that best fits the headline. If a headline fits multiple categories, select the one most relevant. \"\n",
    "            \"Provide only the category name as the response, without any additional text.\"\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "resp = llm.chat(messages)\n",
    "print(f\"System Response: {resp}\")\n",
    "\n",
    "# Chat loop \n",
    "while True:\n",
    "    text_input = input(\"User: \")\n",
    "    if text_input.lower() == \"exit\":\n",
    "        print(\"Exiting classifier. Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    messages.append(ChatMessage(role=\"user\", content=text_input))\n",
    "\n",
    "    response = str(llm.chat(messages))\n",
    "    messages.append(ChatMessage(role='assistant', content=response))    \n",
    "    print(f\"\\nChat: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d765eb0-dcfb-46d9-b2b4-e923e52a289c",
   "metadata": {},
   "source": [
    "## Correctness Evaulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e0d46d5-9bba-4e9f-8f73-a9aa8fc8799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import CorrectnessEvaluator\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "llm = Gemini(\n",
    "    model=\"models/gemini-1.5-flash\",\n",
    "    api_key=GOOGLE_API_KEY  # uses GOOGLE_API_KEY env var by default\n",
    ")\n",
    "\n",
    "evaluator = CorrectnessEvaluator(llm=llm)\n",
    "\n",
    "# Read Data Frame\n",
    "df = pd.read_csv(r'data\\bbc_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586febc8-8771-4293-9fb7-1e9dc764ab73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0f76d87-7235-480d-bf94-775b842b9777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose random entity \n",
    "random_index = random.randint(0, len(df))\n",
    "entity = df.iloc[random_index]\n",
    "\n",
    "news = entity.data\n",
    "label = entity.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990f5cdb-012b-4612-b434-7487fe2b529b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Play around with deepeval library\n",
    "\n",
    "Deepeval problem - I don't have access to GPT models needed to use GEval - useful in correctness measurements\n",
    "\n",
    "ValueError: Invalid model. Available GPT models: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-4-turbo-preview, gpt-4-0125-preview, gpt-4-1106-preview, gpt-4, gpt-4-32k, gpt-4-0613, gpt-4-32k-0613, gpt-3.5-turbo-1106, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "69881961-3e2a-45c2-9d3a-78e5d577034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.dataset import EvaluationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0fe196c1-1b2a-494a-9f51-23bbcca00e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = df.iloc[4]\n",
    "\n",
    "news = entity.data\n",
    "label = entity.labels\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\",\n",
    "        content=(\n",
    "            \"You are a text classifier specializing in BBC headlines. \"\n",
    "            \"I will provide you with a headline, and you will classify it into one of the following five categories: \"\n",
    "            \"business, entertainment, politics, sport, or tech. \"\n",
    "            \"Choose the category that best fits the headline. If a headline fits multiple categories, select the one most relevant. \"\n",
    "            \"Provide only the category name as the response, without any additional text.\"\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "messages.append(ChatMessage(role=\"user\", content=news))\n",
    "ouput = str(llm.chat(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "16d282d8-3343-4321-8cc7-4555a051cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case with a correctness score of 1 (complete alignment with expected output)\n",
    "first_test_case = LLMTestCase(input=news,\n",
    "                              actual_output=ouput,\n",
    "                              expected_output=label)\n",
    "\n",
    "# Test Case with a correctness score of 0.5 (partial alignment with expected output)\n",
    "second_test_case = LLMTestCase(input=news,\n",
    "                               actual_output=ouput,\n",
    "                               expected_output=label)\n",
    "\n",
    "# Test Case with a correctness score of 0 (no meaningful alignment with expected output)\n",
    "third_test_case = LLMTestCase(input=news,\n",
    "                              actual_output=ouput,\n",
    "                              expected_output=label)\n",
    "\n",
    "test_cases = [first_test_case, second_test_case, third_test_case]\n",
    "\n",
    "dataset = EvaluationDataset(test_cases=test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5733f717-99dc-4dfb-a92e-770e7a7aedd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid model. Available GPT models: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-4-turbo-preview, gpt-4-0125-preview, gpt-4-1106-preview, gpt-4, gpt-4-32k, gpt-4-0613, gpt-4-32k-0613, gpt-3.5-turbo-1106, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-0125",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m correctness_metric \u001b[38;5;241m=\u001b[39m \u001b[43mGEval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCorrectness\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemini-1.5-flash\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLLMTestCaseParams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEXPECTED_OUTPUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLLMTestCaseParams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mACTUAL_OUTPUT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDetermine whether the actual output is factually correct based on the expected output.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive - Accenture\\Desktop\\MyFolder\\Studying\\GenAi\\RAG_Testing\\vEnv\\Lib\\site-packages\\deepeval\\metrics\\g_eval\\g_eval.py:87\u001b[0m, in \u001b[0;36mGEval.__init__\u001b[1;34m(self, name, evaluation_params, criteria, evaluation_steps, model, threshold, async_mode, strict_mode, verbose_mode, _include_g_eval_suffix)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluation_steps\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must not be an empty list. Either omit evaluation steps or include a non-empty list of steps.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m     )\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriteria \u001b[38;5;241m=\u001b[39m criteria\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musing_native_model \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_model_name()\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_steps \u001b[38;5;241m=\u001b[39m evaluation_steps\n",
      "File \u001b[1;32m~\\OneDrive - Accenture\\Desktop\\MyFolder\\Studying\\GenAi\\RAG_Testing\\vEnv\\Lib\\site-packages\\deepeval\\metrics\\utils.py:271\u001b[0m, in \u001b[0;36minitialize_model\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# Otherwise (the model is a string or None), we initialize a GPTModel and use as a native model\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGPTModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - Accenture\\Desktop\\MyFolder\\Studying\\GenAi\\RAG_Testing\\vEnv\\Lib\\site-packages\\deepeval\\models\\gpt_model.py:93\u001b[0m, in \u001b[0;36mGPTModel.__init__\u001b[1;34m(self, model, _openai_api_key, base_url, *args, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_use_local_model()\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m model_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valid_gpt_models\n\u001b[0;32m     92\u001b[0m     ):\n\u001b[1;32m---> 93\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid model. Available GPT models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(model\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mvalid_gpt_models)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m         )\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m default_gpt_model\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid model. Available GPT models: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-4-turbo-preview, gpt-4-0125-preview, gpt-4-1106-preview, gpt-4, gpt-4-32k, gpt-4-0613, gpt-4-32k-0613, gpt-3.5-turbo-1106, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-0125"
     ]
    }
   ],
   "source": [
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.EXPECTED_OUTPUT,\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    evaluation_steps=[\n",
    "        \"Determine whether the actual output is factually correct based on the expected output.\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd5c58-4caa-4d35-8c7e-5e5297e5c667",
   "metadata": {},
   "source": [
    "### RAGAS Framework playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd00110-1704-485b-a7b8-0bf64c67e5e4",
   "metadata": {},
   "source": [
    "You can do it when you have Azure OpenAI API:\n",
    "\n",
    "- https://docs.ragas.io/en/v0.1.21/howtos/customisations/azure-openai.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "675821a5-2e7a-46fd-9171-bbc27f2b8ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import  MultiTurnSample, SingleTurnSample\n",
    "from ragas.messages import HumanMessage,AIMessage,ToolMessage,ToolCall\n",
    "from ragas.metrics import AgentGoalAccuracyWithReference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d74146b3-d5c0-4d21-96db-96ec243a94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = df.iloc[4]\n",
    "\n",
    "news = entity.data\n",
    "label = entity.labels\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\",\n",
    "        content=(\n",
    "            \"You are a text classifier specializing in BBC headlines. \"\n",
    "            \"I will provide you with a headline, and you will classify it into one of the following five categories: \"\n",
    "            \"business, entertainment, politics, sport, or tech. \"\n",
    "            \"Choose the category that best fits the headline. If a headline fits multiple categories, select the one most relevant. \"\n",
    "            \"Provide only the category name as the response, without any additional text.\"\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "messages.append(ChatMessage(role=\"user\", content=news))\n",
    "output = str(llm.chat(messages)).replace('\\t','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7468aa62-69f1-4ca6-874a-7952e90ab97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = news\n",
    "\n",
    "# AI's response\n",
    "response = output\n",
    "\n",
    "# Reference answer (ground truth)\n",
    "reference = label\n",
    "\n",
    "# Evaluation rubric\n",
    "rubric = {\n",
    "    \"accuracy\": \"Correct\",\n",
    "    \"completeness\": \"High\",\n",
    "    \"fluency\": \"Excellent\"\n",
    "}\n",
    "\n",
    "# Create the SingleTurnSample instance\n",
    "sample = SingleTurnSample(\n",
    "    user_input=user_input,\n",
    "    response=response,\n",
    "    reference=reference,\n",
    "    rubric=rubric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47369c0-5756-4080-b96c-e754dcdcaca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = AgentGoalAccuracyWithReference()\n",
    "score.llm = llm\n",
    "await score.ascore(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5d870-d6e0-4694-89c5-a010519fbd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = MultiTurnSample(\n",
    "    \n",
    "    user_input=[\n",
    "    HumanMessage(content=\"Hey, book a table at the nearest best Chinese restaurant for 8:00pm\"),\n",
    "    AIMessage(content=\"Sure, let me find the best options for you.\", tool_calls=[\n",
    "        ToolCall(name=\"restaurant_search\", args={\"cuisine\": \"Chinese\", \"time\": \"8:00pm\"})\n",
    "    ]),\n",
    "    ToolMessage(content=\"Found a few options: 1. Golden Dragon, 2. Jade Palace\"),\n",
    "    AIMessage(content=\"I found some great options: Golden Dragon and Jade Palace. Which one would you prefer?\"),\n",
    "    HumanMessage(content=\"Let's go with Golden Dragon.\"),\n",
    "    AIMessage(content=\"Great choice! I'll book a table for 8:00pm at Golden Dragon.\", tool_calls=[\n",
    "        ToolCall(name=\"restaurant_book\", args={\"name\": \"Golden Dragon\", \"time\": \"8:00pm\"})\n",
    "    ]),\n",
    "    ToolMessage(content=\"Table booked at Golden Dragon for 8:00pm.\"),\n",
    "    AIMessage(content=\"Your table at Golden Dragon is booked for 8:00pm. Enjoy your meal!\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "],\n",
    "    reference=\"Table booked at one of the chinese restaurants at 8 pm\")\n",
    "\n",
    "scorer = AgentGoalAccuracyWithReference()\n",
    "scorer.llm = llm\n",
    "await scorer.multi_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cbe96589-81db-4e40-98a4-6d703f101308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LlamaIndexLLMWrapper\n",
    "\n",
    "evaluator_llm = LlamaIndexLLMWrapper(your_llm_instance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b261418b-034b-4cb8-87ef-ca31d43b432d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13718598426177148"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import BleuScore\n",
    "\n",
    "test_data = {\n",
    "    \"user_input\": \"summarise given text\\nThe company reported an 8% rise in Q3 2024, driven by strong performance in the Asian market. Sales in this region have significantly contributed to the overall growth. Analysts attribute this success to strategic marketing and product localization. The positive trend in the Asian market is expected to continue into the next quarter.\",\n",
    "    \"response\": \"The company experienced an 8% increase in Q3 2024, largely due to effective marketing strategies and product adaptation, with expectations of continued growth in the coming quarter.\",\n",
    "    \"reference\": \"The company reported an 8% growth in Q3 2024, primarily driven by strong sales in the Asian market, attributed to strategic marketing and localized products, with continued growth anticipated in the next quarter.\"\n",
    "}\n",
    "metric = BleuScore()\n",
    "test_data = SingleTurnSample(**test_data)\n",
    "metric.single_turn_score(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
